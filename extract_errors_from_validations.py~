from math import sqrt
from numpy import concatenate
import matplotlib.pyplot as plt
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
import numpy as np
import pandas as pd
import cPickle as pickle
import ipdb, os, glob
from collections import OrderedDict
from sklearn import preprocessing
from anomaly_detection_with_prediction import (detect_anomalies_with_prediction_actuals,
                                              plot_anomaly_with_matplotlib)
from rnn_models import (get_model_via_name, series_to_supervised)
import ipromps_lib 
from scipy.interpolate import griddata

def convert_to_X_y(dataset = None, n_lags=None):
    values = dataset.values
    # normalize features
    scaler = MinMaxScaler(feature_range=(-1, 1))
    train_scaled = scaler.fit_transform(values)

    # data is sampling in 50HZ specify the number of lags = 10
    n_features = values.shape[-1]
    # frame as supervised learning
    reframed = series_to_supervised(train_scaled, n_lags, 1)

    # split into train and test sets
    refValues = reframed.values
    
    # split into input and outputs
    n_obs = n_lags * n_features
    X, y = refValues[:, :n_obs], refValues[:, -n_features:]

    # reshape input to be 3D [samples, timesteps, features]
    X = X.reshape((X.shape[0], n_lags, n_features))
    
    return X, y, scaler, n_features

def model_errors_with_prob_by_feature(group_errors_by_csv = None, sampling_rate = 50.0, axarr = None): # 50 HZ
    csvs = group_errors_by_csv.keys()
    lengths = [group_errors_by_csv[csv].shape[0] for csv in csvs]
    regular_length = int(round(np.average(lengths)))
    durations = [length / float(sampling_rate) for length in lengths] # unit: seconds

    # align the time with regular_length
    group_errors_by_csv_aligned = {}
    for i, csv in enumerate(csvs):
        stamp_raw = np.linspace(0, durations[i],lengths[i])
        stamp_aligned = np.linspace(0, stamp_raw[-1], regular_length)
        values_filtered = group_errors_by_csv[csv]
        values_aligned = griddata(stamp_raw, values_filtered, stamp_aligned, method='linear')
        group_errors_by_csv_aligned[csv] = values_aligned

    
    # group_errors_by_feature
    n_features = group_errors_by_csv_aligned[csvs[0]].shape[-1]
    # group_errors_by_feature = OrderedDict()
    # for feature in range(n_features):
    #     errors_by_feature = []
    #     for csv in csvs:
    #         errors_by_feature.append(group_errors_by_csv_aligned[csv][:, feature])
    #     group_errors_by_feature[feature] = errors_by_feature
    
    group_promp_model_by_feature = OrderedDict()
    for feature in range(n_features):
        min_max_scaler = preprocessing.MinMaxScaler()
        promp = ipromps_lib.ProMP(num_basis=35,
                                  sigma_basis=0.05,
                                  num_samples=regular_length,
                                  sigmay=0.0,
                                  min_max_scaler=None,
                                  )
        print('processing dim=%d'%feature)
        for i, csv in enumerate(csvs):
            data = group_errors_by_csv_aligned[csv][:,feature]
            promp.add_demonstration(data)
            promp.add_alpha(durations[i])
        plt.axes(axarr[feature, 1])
        axarr[feature, 1].set_title('probabilistic model of feature #%s'%feature)        
        
        means, stds = promp.plot_prior(b_regression=False, linewidth_mean=3, b_dataset=True)

        # test the new obs, only for temporary testing
        obs = data[:5]
        stamp = np.linspace(0, 5, 5)
        alpha_candi = promp.alpha_candidate(num_alpha_candidate = 6)
        idx_max = promp.estimate_alpha(alpha_candi, obs, stamp)
        print('alpha_candidate_max_idx=%d'%idx_max)
        estimated_alpha = alpha_candi[idx_max]['candidate']
        promp.set_alpha(estimated_alpha)
        print('Adding via points in each promp model...')
        for idx in range(len(stamp)):
            t = stamp[idx] / estimated_alpha
            promp.add_viapoint(t, obs[idx])
            mean = promp.get_mean(t)
            ix = np.where(means == mean)[0][0]
            std = stds[ix]
            print status(e = obs[idx], mean = mean, std = std, c = 2.0)
        promp.plot_uViapoints()
        
        group_promp_model_by_feature[feature] = promp
    return group_promp_model_by_feature


def status(e = None, mean =  None, std = None, c = None):
    upperThreshold = mean + c * std
    lowerThreshold = mean - c * std
    if e > upperThreshold or e < lowerThreshold :
        return "anomalies"
    else:
        return "norminal"
        
if __name__=="__main__":

    curr_path =  os.path.dirname(os.path.realpath(__file__))
    skill = 3
    # load successful dataset for training and validating
    succ_csvs = glob.glob(os.path.join(
        curr_path,
        'successful_skills',
        'skill %s'%skill,
        '*',
        '*.csv'
    ))

    n_lags = 10
    train_dataset = read_csv(succ_csvs[0], header=0, index_col=0)
    train_X, train_y, scaler, n_features = convert_to_X_y(dataset = train_dataset, n_lags=n_lags)    
    input_shape = (train_X.shape[1], train_X.shape[2])
    model_name = "gru_dropout" #"lstm_dropout" "lstm" 
    model = get_model_via_name(model_name= model_name,
                               input_shape = input_shape,
                               output_shape = n_features)
    _weights = "%s_model.h5"%model_name
    print ("Model trained, loading the weights and history...")
    model.load_weights(_weights)
    
    fig, axarr = plt.subplots(nrows=n_features, ncols=2, sharex=False)
    plt.subplots_adjust(hspace=0.5)
    group_errors_by_csv = OrderedDict()
    for i, csv in enumerate(succ_csvs):
        valid_dataset = read_csv(csv, header=0, index_col=0)
        valid_X, valid_y, _, _ = convert_to_X_y(dataset = valid_dataset, n_lags=n_lags)

        # make a prediction
        yhat   = model.predict(valid_X)
        # invert scaling for forecasting
        inv_yhat = scaler.inverse_transform(yhat)
        # invert scaling for actual
        inv_y = scaler.inverse_transform(valid_y)

        for dim in range(n_features):
            _yhat = inv_yhat[:,dim]
            _y = inv_y[:,dim]
            # calculate RMSE
            rmse = sqrt(mean_squared_error(_y,
                                           _yhat))
            print('Test RMSE: %.3f of dim_%s' % (rmse, dim))


            # plot the predict and actual
            # axarr[dim].plot(_y, label='actuals')
            # axarr[dim].plot(_yhat, label='predicted')

            # plot the error by actual - predict
            e = _y - _yhat
            axarr[dim, 0].plot(e, label='errors' if i==0 else "")            
            axarr[dim, 0].legend()
            axarr[dim, 0].set_title('error sequences of feature #%s'%dim)
        fileID = "csv-%s"%i
        group_errors_by_csv[fileID] = inv_y - inv_yhat
    print group_errors_by_csv.keys()

    # build probabilistic model for each feature
    group_promp_model_by_feature = model_errors_with_prob_by_feature(group_errors_by_csv=group_errors_by_csv,
                                                                     sampling_rate = 50,
                                                                     axarr = axarr)

    
    
    
    plt.show()
